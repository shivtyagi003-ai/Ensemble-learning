{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea behind it?\n",
        "- Ensemble Learning is a machine learning paradigm where multiple models (often called \"base learners\" or \"weak learners\") are trained to solve the same problem and combined to yield better results.\n",
        "Key idea behind it is Error Reduction-\n",
        "The fundamental goal of ensemble learning is to reduce the three main types of error that lead to poor model performance: Bias, Variance.\n",
        "Bias (Underfitting): When a model is too simple to capture the underlying patterns of the data.\n",
        "\n",
        "Variance (Overfitting): When a model is too sensitive to the specific noise in the training data and fails to generalize to new data.\n",
        "\n",
        "2. What is the difference between Bagging and Boosting?\n",
        "- Bagging:- Bagging, short for Bootstrap Aggregating, is an ensemble machine learning technique designed to improve the stability and accuracy of algorithms by reducing variance.\n",
        "Boosting:- Boosting is an ensemble machine learning technique that combines a series of \"weak learners\" (models that are only slightly better than random guessing) into a single \"strong learner.\"\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "- Bootstrap Sampling is a statistical technique used to estimate the distribution of a population by repeatedly sampling from a single dataset with replacement.\n",
        "The Role of Bootstrapping in Random Forest:-\n",
        "1. Decoupling the Trees (Independency)- Each tree in a Random Forest is trained on its own unique bootstrap sample. Because each tree sees a slightly different version of the data, they develop different \"opinions.\n",
        "2. Handling Outliers- If your dataset has a \"weird\" outlier, bootstrapping ensures that only some trees see that outlier.\n",
        "\n",
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "- Out-of-Bag (OOB) samples are the data points \"left over\" during the bootstrap sampling process in Bagging methods like Random Forest. Because they were never shown to a specific tree during its training, they act as a natural, \"built-in\" test set for that individual tree.\n",
        "\n",
        "5.  Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest?\n",
        "- 1. The Single Decision Tree:\n",
        "In a single tree, feature importance is calculated based on which features are used to split the data at each node.\n",
        "-> Logic: The feature that provides the greatest reduction in Gini impurity or Entropy (at the root) is often considered the most important.\n",
        "-> Stability (Low): If you change your dataset even slightly, the root node might change entirely. This makes the feature importance ranking very volatile—it can flip-flop based on minor noise in the data.\n",
        "\n",
        "2. The Random Forest:\n",
        "-> Logic: Since each tree is built on a different random subset of data (Bagging) and a different random subset of features, many different variables get a \"chance to shine.\"\n",
        "-> Stability (High): Because it averages hundreds of trees, the importance scores are much more stable. Adding or removing a few rows of data won't significantly change the rankings.\n",
        "\n",
        "6.\n",
        "\n"
      ],
      "metadata": {
        "id": "CampFhvHFFHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VM1_CLMNFCpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c744a9ce-0929-4bcf-f861-7cc840da1fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "# Write a Python program to:\n",
        "# Load the Breast Cancer dataset using\n",
        "#sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "feature_names = data.feature_names\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values(\n",
        "    by='Importance', ascending=False\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to:\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "dt_predictions = dt.predict(X_test)\n",
        "\n",
        "\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "bag_predictions = bagging.predict(X_test)\n",
        "\n",
        "bag_accuracy = accuracy_score(y_test, bag_predictions)\n",
        "\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb8P-X8cKEDB",
        "outputId": "f47f85b1-ac00-4533-b102-dfe4b75d76a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "#● Train a Random Forest Classifier\n",
        "#● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy:\", final_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFyJZDKSKmoN",
        "outputId": "3617e972-4547-4422-fdab-2bf9952b24b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to:\n",
        "#● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "#● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "bagging_predictions = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "\n",
        "\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "rf_predictions = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQy39r1PLdsH",
        "outputId": "de5eb9d7-797d-4005-e892-4f6ef08e9673"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2568358813508342\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCisOUE3L-S2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}